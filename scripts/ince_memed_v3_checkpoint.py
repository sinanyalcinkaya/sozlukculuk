#!/usr/bin/env python3
"""
Ä°nce Memed SÃ¶z VarlÄ±ÄŸÄ± Ã‡Ä±karÄ±cÄ± v3 - Checkpoint Edition

Yeni Ã¶zellikler:
- Her 10 cÃ¼mlede otomatik kayÄ±t (checkpoint)
- Varolan dosyadan devam etme (resume)
- Hata durumunda kaldÄ±ÄŸÄ± yerden devam

DeÄŸiÅŸiklikler:
- System prompt Modelfile'da gÃ¶mÃ¼lÃ¼ (yasar-sozluk modeli)
- Her cÃ¼mle tek tek iÅŸlenir (karÄ±ÅŸma yok)
- Word boundary kontrolÃ¼ (halÃ¼sinasyon Ã¶nleme)
- PDF sayfa numarasÄ± (kitap sayfa no yerine)

Kurulum:
    ollama create yasar-sozluk -f YasarKemalSozluk.modelfile

KullanÄ±m:
    # HÄ±zlÄ± test (ilk 10 cÃ¼mle)
    python ince_memed_v3_checkpoint.py --test-sentences 10
    
    # Sayfa testi
    python ince_memed_v3_checkpoint.py --test 5
    
    # Tam Ã§alÄ±ÅŸtÄ±rma
    python ince_memed_v3_checkpoint.py --full
"""

import argparse
import json
import os
import re
import sys
import time
from pathlib import Path
from dataclasses import dataclass, field

import pdfplumber
import ollama

# ============== CONFIGURATION ==============

@dataclass
class Config:
    model: str = "yasar-sozluk"  # Modelfile ile oluÅŸturulan Ã¶zel model
    temperature: float = 0.2
    checkpoint_interval: int = 10  # Her kaÃ§ cÃ¼mlede bir kayÄ±t
    
    # Stop list
    stop_words: set = field(default_factory=lambda: {
        # Edatlar
        'iÃ§in', 'gibi', 'ile', 'kadar', 'Ã¼zere', 'doÄŸru', 'karÅŸÄ±', 'gÃ¶re',
        'dek', 'deÄŸin', 'beri', 'yana', 'raÄŸmen', 'karÅŸÄ±n', 'Ã¶nce', 'sonra',
        'dolayÄ±', 'Ã¶tÃ¼rÃ¼', 'Ã¼zerine', 'hakkÄ±nda', 'dair',
        # BaÄŸlaÃ§lar
        've', 'veya', 'ya', 'yahut', 'ama', 'fakat', 'ancak', 'lakin',
        'oysa', 'halbuki', 'Ã§Ã¼nkÃ¼', 'zira', 'ki', 'de', 'da', 'bile',
        'dahi', 'hem', 'ne', 'ise', 'madem', 'eÄŸer', 'ÅŸayet', 'yani',
        # Zamirler
        'ben', 'sen', 'biz', 'siz', 'bu', 'ÅŸu', 'o', 'bunlar', 'ÅŸunlar',
        'onlar', 'kim', 'ne', 'hangi', 'kendi', 'hep', 'hiÃ§',
        # Soru / belirsizlik
        'mÄ±', 'mi', 'mu', 'mÃ¼', 'deÄŸil',
        'bir', 'her', 'bazÄ±', 'birkaÃ§', 'hiÃ§bir',
        # Derece zarflarÄ±
        'daha', 'Ã§ok', 'pek', 'en', 'az', 'biraz', 'epey', 'gayet', 'fazla',
    })

CONFIG = Config()

# ============== PDF EXTRACTION ==============

def preprocess_text(text: str) -> str:
    """OCR dÃ¼zeltmeleri"""
    text = text.replace('Ä', 'Ä°').replace('Ä‘', 'i')
    text = text.replace('Ñ€', 'r').replace('Ğ°', 'a')  # Kiril
    text = re.sub(r'[ \t]+', ' ', text)
    return text


def extract_sentences_from_pdf(pdf_path: str, start_page: int = 0, end_page: int = None, 
                                max_sentences: int = None) -> list[dict]:
    """
    PDF'den cÃ¼mleleri Ã§Ä±kar.
    Returns: [{"pdf_sayfa": 5, "cumle": "..."}, ...]
    """
    sentences = []
    sentence_id = 0
    
    with pdfplumber.open(pdf_path) as pdf:
        if end_page is None:
            end_page = len(pdf.pages)
        
        for page_idx in range(start_page, min(end_page, len(pdf.pages))):
            text = pdf.pages[page_idx].extract_text() or ""
            if not text.strip():
                continue
            
            pdf_sayfa = page_idx + 1  # 1-indexed
            
            # Pre-processing
            text = preprocess_text(text)
            
            # SatÄ±r sonu tire birleÅŸtirme
            text = re.sub(r'(\w)-\s*\n\s*(\w)', r'\1\2', text)
            
            # SatÄ±r iÃ§i sayfa numaralarÄ±nÄ± temizle
            text = re.sub(r'\n\s*\d{1,4}\s*\n', '\n', text)
            
            # SatÄ±r sonlarÄ±nÄ± boÅŸluÄŸa Ã§evir
            text = re.sub(r'\n+', ' ', text)
            text = re.sub(r'\s+', ' ', text)
            
            # CÃ¼mlelere bÃ¶l
            raw_sentences = re.split(r'(?<=[.!?])\s+', text)
            
            for sent in raw_sentences:
                sent = sent.strip()
                # GeÃ§erli cÃ¼mle mi?
                if len(sent) > 15 and re.search(r'[a-zA-ZÃ§Ã‡ÄŸÄÄ±Ä°Ã¶Ã–ÅŸÅÃ¼Ãœ]{4,}', sent):
                    sentence_id += 1
                    sentences.append({
                        "cumle_id": sentence_id,
                        "pdf_sayfa": pdf_sayfa,
                        "cumle": sent
                    })
                    
                    # Max sentence kontrolÃ¼
                    if max_sentences and len(sentences) >= max_sentences:
                        return sentences
    
    return sentences


# ============== LLM PROCESSING ==============

def process_single_sentence(sentence: str, model: str) -> dict:
    """
    Tek cÃ¼mleyi iÅŸle. System prompt modelde gÃ¶mÃ¼lÃ¼.
    """
    try:
        response = ollama.chat(
            model=model,
            messages=[
                {"role": "user", "content": sentence}
            ],
            format="json",
            options={
                "temperature": CONFIG.temperature,
                "num_predict": 1500
            }
        )
        
        raw_output = response['message']['content']
        
        try:
            data = json.loads(raw_output)
            return {
                "success": True,
                "tokens": data.get("tokens", []),
                "raw": raw_output
            }
        except json.JSONDecodeError as e:
            return {"success": False, "error": f"JSON parse: {e}", "raw": raw_output}
            
    except Exception as e:
        return {"success": False, "error": str(e), "raw": ""}


def validate_token_in_sentence(token: str, sentence: str) -> bool:
    """
    Token cÃ¼mlede KELIME olarak var mÄ±? (substring deÄŸil)
    """
    token_lower = token.lower().strip()
    sentence_lower = sentence.lower()
    
    # BoÅŸ token
    if not token_lower:
        return False
    
    # Word boundary ile kontrol
    # TÃ¼rkÃ§e karakterleri de kelime sÄ±nÄ±rÄ± olarak kabul et
    pattern = r'(?<![a-zA-ZÃ§Ã‡ÄŸÄÄ±Ä°Ã¶Ã–ÅŸÅÃ¼Ãœ])' + re.escape(token_lower) + r'(?![a-zA-ZÃ§Ã‡ÄŸÄÄ±Ä°Ã¶Ã–ÅŸÅÃ¼Ãœ])'
    return bool(re.search(pattern, sentence_lower))


def filter_and_validate_tokens(tokens: list[dict], sentence: str) -> list[dict]:
    """Token'larÄ± filtrele ve doÄŸrula"""
    validated = []
    
    for t in tokens:
        token_text = t.get("token", "").strip()
        lemma = t.get("lemma", "").lower().rstrip("-")
        
        # Stop word kontrolÃ¼
        if lemma in CONFIG.stop_words or token_text.lower() in CONFIG.stop_words:
            continue
        
        # Ã‡ok kÄ±sa
        if len(token_text) < 2:
            continue
        
        # Sadece noktalama
        if re.match(r'^[.,!?;:"\'\-]+$', token_text):
            continue
        
        # CÃ¼mlede gerÃ§ekten var mÄ±?
        if not validate_token_in_sentence(token_text, sentence):
            continue
        
        validated.append(t)
    
    return validated


# ============== MAIN PROCESSOR ==============

class SozVarligiProcessor:
    def __init__(self, model: str = CONFIG.model, output_prefix: str = "ince_memed_sozluk"):
        self.model = model
        self.output_prefix = output_prefix
        self.results = []
        self.cumle_counter = 0
        self.processed_count = 0  # Bu session'da iÅŸlenen cÃ¼mle sayÄ±sÄ±
        self.stats = {
            "toplam_cumle": 0,
            "toplam_token": 0,
            "basarili_cumle": 0,
            "hatali_cumle": 0,
            "etiket_dagilimi": {}
        }
    
    def load_checkpoint(self, json_file: str) -> bool:
        """Varolan checkpoint'i yÃ¼kle"""
        if not os.path.exists(json_file):
            return False
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.results = data.get("data", [])
            self.stats = data.get("meta", {}).get("stats", self.stats)
            
            # Cumle counter'Ä± gÃ¼ncelle
            if self.results:
                self.cumle_counter = max(r["cumle_id"] for r in self.results)
            
            print(f"ğŸ“‚ Checkpoint yÃ¼klendi: {len(self.results)} kayÄ±t, son ID: {self.cumle_counter}")
            return True
        except Exception as e:
            print(f"âš ï¸  Checkpoint yÃ¼kleme hatasÄ±: {e}")
            return False
    
    def get_processed_sentence_ids(self) -> set:
        """Ä°ÅŸlenmiÅŸ cÃ¼mle ID'lerini dÃ¶ndÃ¼r"""
        return {r["cumle_id"] for r in self.results}
    
    def save_checkpoint(self):
        """Mevcut durumu kaydet"""
        json_file = f"{self.output_prefix}.json"
        tsv_file = f"{self.output_prefix}.tsv"
        
        self.export_json(json_file, silent=True)
        self.export_tsv(tsv_file, silent=True)
        print(f"      ğŸ’¾ Checkpoint kaydedildi ({len(self.results)} kayÄ±t)")
    
    def process_sentence(self, sent_data: dict) -> dict:
        """Tek cÃ¼mle iÅŸle"""
        pdf_sayfa = sent_data["pdf_sayfa"]
        cumle = sent_data["cumle"]
        cumle_id = sent_data["cumle_id"]
        
        result = process_single_sentence(cumle, self.model)
        
        if result["success"]:
            tokens = filter_and_validate_tokens(result.get("tokens", []), cumle)
            
            # Etiket istatistiÄŸi
            for t in tokens:
                etiket = t.get("etiket", "") or "STANDART"
                self.stats["etiket_dagilimi"][etiket] = \
                    self.stats["etiket_dagilimi"].get(etiket, 0) + 1
            
            self.stats["basarili_cumle"] += 1
            self.stats["toplam_token"] += len(tokens)
            
            return {
                "pdf_sayfa": pdf_sayfa,
                "cumle_id": cumle_id,
                "cumle": cumle,
                "tokens": tokens
            }
        else:
            self.stats["hatali_cumle"] += 1
            return None
    
    def process_sentences(self, sentences: list[dict], verbose: bool = True):
        """CÃ¼mle listesini iÅŸle"""
        # Checkpoint yÃ¼kle
        json_file = f"{self.output_prefix}.json"
        checkpoint_loaded = self.load_checkpoint(json_file)
        
        processed_ids = self.get_processed_sentence_ids()
        
        # HenÃ¼z iÅŸlenmemiÅŸ cÃ¼mleleri filtrele
        remaining_sentences = [s for s in sentences if s["cumle_id"] not in processed_ids]
        
        if checkpoint_loaded and not remaining_sentences:
            print(f"âœ… TÃ¼m cÃ¼mleler zaten iÅŸlenmiÅŸ!")
            self.print_stats()
            return
        
        total = len(sentences)
        remaining_count = len(remaining_sentences)
        already_processed = total - remaining_count
        
        start_time = time.time()
        
        print(f"\nğŸš€ Ä°ÅŸlem baÅŸlÄ±yor...")
        print(f"   Model: {self.model}")
        print(f"   Toplam cÃ¼mle: {total}")
        if checkpoint_loaded:
            print(f"   âœ… Zaten iÅŸlenmiÅŸ: {already_processed}")
            print(f"   ğŸ”„ Ä°ÅŸlenecek: {remaining_count}")
        print("=" * 60)
        
        for i, sent_data in enumerate(remaining_sentences):
            sent_start = time.time()
            
            result = self.process_sentence(sent_data)
            
            sent_time = time.time() - sent_start
            elapsed = time.time() - start_time
            
            if result and result["tokens"]:
                self.results.append(result)
            
            self.processed_count += 1
            
            if verbose:
                status = f"âœ… {len(result['tokens'])} token" if result else "âŒ Hata"
                eta = (elapsed / self.processed_count) * (remaining_count - self.processed_count)
                
                cumle_short = sent_data["cumle"][:50]
                current_index = already_processed + self.processed_count
                print(f"[{current_index}/{total}] {status} ({sent_time:.1f}s) | "
                      f"S.{sent_data['pdf_sayfa']} | {cumle_short}...")
                
                # Her 10 cÃ¼mlede checkpoint kaydet
                if self.processed_count % CONFIG.checkpoint_interval == 0:
                    self.save_checkpoint()
                    print(f"      ğŸ“Š Toplam: {len(self.results)} kayÄ±t, "
                          f"{self.stats['toplam_token']} token | ETA: {eta:.0f}s")
        
        # Son checkpoint
        self.save_checkpoint()
        
        self.stats["toplam_cumle"] = total
        
        total_time = time.time() - start_time
        print("\n" + "=" * 60)
        print(f"âœ… TAMAMLANDI! {total_time:.1f} saniye")
        self.print_stats()
    
    def print_stats(self):
        """Ä°statistikleri yazdÄ±r"""
        print(f"\nğŸ“Š Ä°STATÄ°STÄ°KLER")
        print(f"   Toplam cÃ¼mle: {self.stats['toplam_cumle']}")
        print(f"   BaÅŸarÄ±lÄ±: {self.stats['basarili_cumle']}")
        print(f"   HatalÄ±: {self.stats['hatali_cumle']}")
        print(f"   Toplam token: {self.stats['toplam_token']}")
        print(f"   Toplam kayÄ±t: {len(self.results)}")
        print(f"\n   Etiket daÄŸÄ±lÄ±mÄ±:")
        for etiket, count in sorted(self.stats["etiket_dagilimi"].items(), 
                                     key=lambda x: -x[1]):
            print(f"      {etiket or '(boÅŸ)'}: {count}")
    
    def export_json(self, output_file: str, silent: bool = False):
        """JSON olarak dÄ±ÅŸa aktar"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "meta": {
                    "model": self.model,
                    "stats": self.stats
                },
                "data": self.results
            }, f, ensure_ascii=False, indent=2)
        if not silent:
            print(f"\nğŸ“ JSON: {output_file}")
    
    def export_tsv(self, output_file: str, silent: bool = False):
        """TSV olarak dÄ±ÅŸa aktar"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("pdf_sayfa\tcumle_id\ttoken\tlemma\tanlam\tetiket\tcumle\n")
            
            for record in self.results:
                for token in record["tokens"]:
                    cumle_clean = record["cumle"].replace("\t", " ").replace("\n", " ")[:100]
                    f.write(f"{record['pdf_sayfa']}\t"
                           f"{record['cumle_id']}\t"
                           f"{token.get('token', '')}\t"
                           f"{token.get('lemma', '')}\t"
                           f"{token.get('anlam', '')}\t"
                           f"{token.get('etiket', '')}\t"
                           f"{cumle_clean}\n")
        
        if not silent:
            print(f"ğŸ“ TSV: {output_file}")


# ============== CLI ==============

def main():
    parser = argparse.ArgumentParser(description='Ä°nce Memed SÃ¶z VarlÄ±ÄŸÄ± Ã‡Ä±karÄ±cÄ± v3 - Checkpoint Edition')
    
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--test-sentences', type=int, metavar='N',
                       help='Test: ilk N cÃ¼mleyi iÅŸle')
    group.add_argument('--test', type=int, metavar='N',
                       help='Test: ilk N PDF sayfasÄ±nÄ± iÅŸle')
    group.add_argument('--full', action='store_true',
                       help='Tam Ã§alÄ±ÅŸtÄ±rma')
    
    parser.add_argument('--input', '-i',
                        default='ince_memed.pdf',
                        help='GiriÅŸ PDF dosyasÄ±')
    parser.add_argument('--output', '-o', default='ince_memed_sozluk',
                        help='Ã‡Ä±kÄ±ÅŸ dosya adÄ± (uzantÄ±sÄ±z)')
    parser.add_argument('--model', '-m', default=CONFIG.model,
                        help=f'Ollama model (default: {CONFIG.model})')
    parser.add_argument('--checkpoint-interval', type=int, default=CONFIG.checkpoint_interval,
                        help=f'Her kaÃ§ cÃ¼mlede checkpoint (default: {CONFIG.checkpoint_interval})')
    
    args = parser.parse_args()
    
    # Checkpoint interval gÃ¼ncelle
    CONFIG.checkpoint_interval = args.checkpoint_interval
    
    # Model kontrolÃ¼
    print(f"ğŸ” Model kontrol: {args.model}")
    try:
        ollama.show(args.model)
        print(f"   âœ… Model mevcut")
    except:
        print(f"   âŒ Model bulunamadÄ±!")
        print(f"   Ã–nce modeli oluÅŸturun:")
        print(f"   ollama create yasar-sozluk -f YasarKemalSozluk.modelfile")
        sys.exit(1)
    
    processor = SozVarligiProcessor(model=args.model, output_prefix=args.output)
    
    if args.test_sentences:
        print(f"\nğŸ§ª TEST: Ä°lk {args.test_sentences} cÃ¼mle")
        sentences = extract_sentences_from_pdf(args.input, max_sentences=args.test_sentences)
        processor.process_sentences(sentences)
        
    elif args.test:
        print(f"\nğŸ§ª TEST: Ä°lk {args.test} PDF sayfasÄ±")
        sentences = extract_sentences_from_pdf(args.input, start_page=0, end_page=args.test)
        processor.process_sentences(sentences)
        
    elif args.full:
        print("\nğŸš€ TAM Ã‡ALIÅTIRMA")
        sentences = extract_sentences_from_pdf(args.input)
        processor.process_sentences(sentences)


if __name__ == '__main__':
    main()
